{
  "hash": "4be15da443366b26f833d402aff92e84",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Building a Bayesian Learning Agent That Teaches Itself to Eat\"\nsubtitle: \"Watching conjugate priors converge in real-time through Thompson Sampling\"\ndescription: \"A visual demonstration of Bayesian inference and the exploration-exploitation tradeoff. An agent navigates a grid world, learning which foods are safe through exact conjugate updates.\"\nauthor: \"Guy Freeman\"\ndate: 2025-12-26\ncategories: [python, bayesian, machine-learning, simulation, education]\nimage: og-image.png\nexecute:\n  eval: false\n  echo: true\n---\n\nImagine you're stranded in an alien environment with 12 different types of food scattered around. Some combinations of shape and color provide energy, others are toxic. You start with limited energy and lose a bit with every step you take. How do you learn which foods are safe before you run out of energy?\n\nThis is the exploration-exploitation tradeoff in its purest form. Pure exploration—trying everything randomly—will kill you. Pure exploitation—eating only what you *think* is best based on limited data—will starve you when better options exist. You need a strategy that balances both intelligently.\n\nBayesian inference gives us an elegant solution. I built a simulation to *see* this learning process unfold in real-time: an agent navigating a grid world, updating its beliefs about food types using exact Bayesian inference, and making decisions through Thompson Sampling. No neural networks, no reinforcement learning algorithms, just probability theory and about 200 lines of Python.\n\n## The Demo\n\nThe simulation runs in your terminal using curses. Watch the agent (@) navigate a 30×15 grid filled with foods represented by shapes (●, ■, ▲) in different colors. A live belief table shows what the agent has learned about each food type—higher numbers mean more energy, negative numbers mean toxic.\n\n![Simulation Demo](demo.gif)\n\nAs the agent explores, you can watch uncertainty collapse. Belief bars narrow, means shift toward true values, and the agent's behavior evolves from random wandering to purposeful pursuit of high-energy foods. It's Bayesian inference happening in real-time.\n\n## The Problem: Exploration vs. Exploitation\n\nThe agent starts with 10 energy units and loses 0.1 energy per step. It can see nearby food items and must decide which to pursue. There are 12 food types total: 3 shapes (circle, square, triangle) × 4 colors (red, green, blue, yellow). Each food type has an unknown true energy distribution—some are consistently good (circles), some are reliably toxic (triangles), and some are risky gambles (squares).\n\nThe challenge: how do you decide which food to eat when you don't know which types are safe?\n\nHere's where Thompson Sampling comes in. Instead of using fixed exploration rates or complex formulas, it uses a beautifully simple idea: sample from your posterior belief about each food type, then pick the one with the highest sampled value.\n\n::: {#07d2e9c9 .cell execution_count=1}\n``` {.python .cell-code}\ndef select_target_food(self, available_foods):\n    \"\"\"Select food using Thompson Sampling.\"\"\"\n    best_value = -float('inf')\n    best_position = None\n\n    for position, shape, color in available_foods:\n        # Sample energy from our belief distribution\n        belief = self.beliefs[(shape, color)]\n        sampled_energy = np.random.normal(belief[\"mean\"],\n                                         np.sqrt(belief[\"variance\"]))\n\n        # Account for distance cost\n        distance = abs(position[0] - self.position[0]) + \\\n                  abs(position[1] - self.position[1])\n        value = sampled_energy - distance * MOVEMENT_COST\n\n        if value > best_value:\n            best_value = value\n            best_position = position\n\n    return best_position\n```\n:::\n\n\nThe elegance is in what this achieves:\n\n- **High uncertainty = wide distribution = exploration**: When you haven't tried a food much, its belief distribution is wide. Sometimes you'll sample extreme values, causing you to try it.\n- **Low uncertainty = narrow distribution = exploitation**: Once you've learned a food's true value, the distribution narrows. Samples consistently reflect the true mean.\n- **No hyperparameters**: The uncertainty itself controls the exploration rate. It's adaptive and automatic.\n\n## The Math: Conjugate Priors Made Simple\n\nEach food type has an unknown true energy value. The agent maintains a belief distribution over what that value might be, represented as a Normal distribution with mean μ (expected energy), variance σ² (uncertainty), and pseudo-observation count n (effective sample size).\n\nInitially, the agent knows nothing:\n\n```python\n# Prior belief for each food type\nμ₀ = 0.0      # Neutral expectation\nσ₀² = 10.0    # High uncertainty\nn₀ = 0.1      # Weak prior (easily overridden)\n```\n\nWhen the agent eats a food and observes its energy, it updates its belief using **conjugate Bayesian inference**. This is exact mathematics, not an approximation:\n\n::: {#cfe3cd62 .cell execution_count=2}\n``` {.python .cell-code}\ndef update_belief(self, shape, color, observed_energy):\n    \"\"\"\n    Update belief using exact Bayesian inference (Normal-Normal conjugate).\n\n    Prior: μ ~ N(μ₀, σ₀²)\n    Likelihood: x ~ N(μ, σ²)\n    Posterior: μ ~ N(μ₁, σ₁²)\n    \"\"\"\n    belief = self.beliefs[(shape, color)]\n\n    # Prior parameters\n    prior_mean = belief[\"mean\"]\n    prior_variance = belief[\"variance\"]\n    n = belief[\"n\"]\n\n    # Weights for combining prior and observation\n    observation_weight = 1.0\n    total_weight = n + observation_weight\n\n    # Posterior mean: weighted average of prior and observation\n    new_mean = (n * prior_mean + observation_weight * observed_energy) / \\\n               total_weight\n\n    # Posterior variance: always decreases with more data\n    new_variance = prior_variance / (1 + observation_weight / n)\n\n    # Update belief\n    belief[\"mean\"] = new_mean\n    belief[\"variance\"] = new_variance\n    belief[\"n\"] = n + observation_weight\n```\n:::\n\n\nThis is the mathematical heart of the system. The formula elegantly captures two key insights:\n\n1. **The posterior mean is a weighted average**: Trust old beliefs proportionally less as you gather new evidence. Early observations have huge impact; later ones refine.\n\n2. **Uncertainty always decreases**: With each observation, the variance shrinks. After one observation of a food type, variance drops from 10.0 to about 9.1. After ten observations, it's down to 0.9. The agent becomes more confident in what it knows.\n\nNo MCMC sampling, no variational inference, no neural network approximations. This is the true Bayesian posterior because Normal-Normal is a conjugate pair. The math just works.\n\n## Thompson Sampling: The Exploration Strategy\n\nWhy Thompson Sampling instead of other exploration strategies?\n\n**Epsilon-greedy** uses a fixed exploration rate: with probability ε, try something random; otherwise, exploit. The problem is that ε is arbitrary. Too high and you waste time on known bad options. Too low and you miss discovering better choices. And it doesn't adapt—you explore the same amount regardless of uncertainty.\n\n**Upper Confidence Bound (UCB)** uses a deterministic formula: `value = μ + c·σ - distance_cost`. It picks the option with highest upper confidence bound. UCB is provably good, but the constant c needs tuning, and it explores more rigidly than Thompson.\n\n**Thompson Sampling** just samples from your posterior and picks the max:\n\n::: {#9600d8c7 .cell execution_count=3}\n``` {.python .cell-code}\n# This is literally the entire exploration strategy\nsampled_energy = np.random.normal(belief[\"mean\"], np.sqrt(belief[\"variance\"]))\n```\n:::\n\n\nThat's it. Sample from your posterior, account for travel cost, pick the highest sampled value. The mathematics handles exploration automatically.\n\nEarly on, when beliefs are uncertain, you have wide distributions. Sampling from them occasionally gives extreme values, causing exploration of uncertain options. As beliefs sharpen, distributions narrow, samples become consistent, and behavior converges to pure exploitation. The uncertainty itself controls the tradeoff—no hyperparameters needed.\n\nThompson Sampling is also provably optimal for multi-armed bandits with logarithmic regret O(log T). It's not a heuristic; it's a principled strategy that matches the probability of each option being best.\n\n## Implementation Highlights\n\nThe implementation separates concerns cleanly: the agent maintains beliefs and makes decisions, the environment handles world state and dynamics, and the main loop orchestrates everything with curses rendering.\n\n**Grid World Setup**:\n\nThe environment defines ground truth energy distributions for each food type. These are what the agent is trying to learn:\n\n::: {#41c3d855 .cell execution_count=4}\n``` {.python .cell-code}\n# Example ground truth distributions in config.py\nENERGY_DISTRIBUTIONS = {\n    (\"○\", \"red\"): (3.0, 1.0),     # Circle + red: consistently good\n    (\"○\", \"green\"): (2.0, 1.0),   # Circle + green: pretty good\n    (\"■\", \"red\"): (-1.0, 2.0),    # Square + red: risky, often toxic\n    (\"■\", \"blue\"): (2.5, 1.5),    # Square + blue: high variance gamble\n    (\"▲\", \"red\"): (-2.0, 1.0),    # Triangle + red: reliably toxic\n    # ... 7 more food types\n}\n```\n:::\n\n\nCircles tend to be positive, triangles tend to be negative, squares are mixed and risky. The agent doesn't know these values—it must infer them through experience.\n\n**The Learning Loop**:\n\nThe main simulation ties everything together:\n\n::: {#433c36ba .cell execution_count=5}\n``` {.python .cell-code}\n# Simplified learning loop from main.py\nwhile agent.energy > 0 and steps < max_steps:\n    # Agent perceives nearby foods\n    visible_foods = world.get_nearby_foods(agent.position, radius=5)\n\n    if visible_foods:\n        # Thompson Sampling: pick target\n        target = agent.select_target_food(visible_foods)\n\n        # Move toward target\n        agent.move_toward(target)\n\n        # Try to eat at current position\n        consumed = world.consume_food(agent.position)\n        if consumed:\n            shape, color, energy = consumed\n            agent.energy += energy\n\n            # Bayesian update: exact conjugate inference\n            agent.update_belief(shape, color, energy)\n    else:\n        # No food visible, explore randomly\n        agent.move_random()\n\n    # Lose energy for movement\n    agent.energy -= MOVEMENT_COST\n    steps += 1\n\n    # Render to terminal\n    renderer.draw(world, agent)\n```\n:::\n\n\nPerceive → Decide → Act → Learn → Repeat. The Bayesian update is a single method call that performs exact inference. No training epochs, no convergence checks, just math.\n\n**Curses Visualization**:\n\nThe terminal UI shows the grid world, the agent's position, all food items with their shapes and colors, and a live belief summary that updates after each observation. Watch the belief bars narrow and means shift as the agent learns. It's deeply satisfying to see uncertainty collapse in real-time.\n\n## What I Learned Building This\n\n**Conjugate priors are underrated**. In the era of deep learning and approximate inference, exact Bayesian updates feel like a superpower. No training epochs, no learning rate schedules, no convergence diagnostics. You observe data, update your belief with one formula, and you're done. The posterior is exact, not approximate. When you have conjugate pairs, use them.\n\n**Visualization changes understanding**. Reading about Thompson Sampling in a textbook gave me the theory. Watching this agent explore gave me the intuition. Seeing those belief bars narrow, watching the agent shift from random wandering to purposeful pursuit, observing how it balances trying new foods with exploiting known good ones—that's when I *got* it. Abstract concepts become concrete when you can see them unfold.\n\n**Simple simulations teach complex concepts**. This could be a teaching tool for intro ML courses. The grid world is arbitrary, the food types are made up, but the learning principles are universal. Bayesian inference, Thompson Sampling, exploration-exploitation tradeoffs, conjugate updates—all demonstrated in under 200 lines of code with zero ML frameworks. Sometimes simple is better.\n\n**Pure Python + NumPy is enough**. No TensorFlow, PyTorch, or JAX required. No GPU needed. The entire agent is mathematically exact using basic NumPy operations. This is a good reminder that not every ML problem needs deep learning. Some problems have closed-form solutions, and when they do, they're elegant.\n\n## Extensions and Next Steps\n\nThe framework is clean and extensible. Here are natural next steps:\n\n**Environmental extensions**:\n- **Non-stationary environments**: Make food distributions drift over time, forcing the agent to adapt using discounted updates\n- **Spatial correlations**: Food quality depends on location—agents learn regional patterns\n- **Obstacles and pathfinding**: Add walls, implement A* pathfinding to navigate around them\n\n**Multi-agent scenarios**:\n- Competition: Multiple agents sharing resources\n- Cooperation: Agents can share observations to learn faster\n- Emergent behavior: Watch strategies evolve through interaction\n\n**Algorithmic variations**:\n- **Variance learning**: Use Normal-Gamma conjugate to learn both mean and variance\n- **Contextual bandits**: Food value depends on agent state (time of day, current energy)\n- **Beta-Bernoulli version**: Binary rewards (safe/toxic) instead of continuous energy values\n\n**Educational tools**:\n- Jupyter notebook with interactive plots showing belief evolution\n- Side-by-side comparison: Thompson Sampling vs. epsilon-greedy vs. UCB vs. random\n- Regret curve visualization—cumulative energy vs. oracle with perfect knowledge\n- Parameter sensitivity analysis—how does prior strength affect learning speed?\n\nThe codebase is designed for experimentation. Adding new food types takes three lines in the config. Switching from Thompson to UCB is a single boolean flag. The Bayesian update is a standalone method you can modify to try different conjugate priors.\n\n## Conclusion\n\nThis project started as \"I want to understand Thompson Sampling.\" It ended as a visual demonstration of Bayesian inference in action, a teaching tool for probabilistic machine learning, and a reminder that some problems have exact solutions.\n\nThe agent's journey from ignorance to competence mirrors real learning. It starts uncertain about everything, makes mistakes, gradually builds knowledge from experience, and eventually acts with confidence. No pre-training, no labeled data, no reward engineering. Just observations and Bayes' rule.\n\nThere's something deeply satisfying about watching an agent learn from scratch. The mathematics is beautiful because it's both principled (provably optimal regret bounds) and practical (runs in a terminal with NumPy). Code can be poetry when it's this clean.\n\nThe code is on [GitHub](https://github.com/gfrmin/bayesian-agent). Try it yourself:\n\n```bash\ngit clone https://github.com/gfrmin/bayesian-agent\ncd bayesian-agent\nuv run main.py\n```\n\nFork it, extend it, teach with it. Add new food types, try different priors, implement multi-agent scenarios, compare strategies. The framework is there. The math is exact. The learning is real.\n\nWatch an agent learn, and maybe learn something yourself about how learning works.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}